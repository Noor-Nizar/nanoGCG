import copy
import gc
import logging

from dataclasses import dataclass
from tqdm import tqdm
from typing import List, Optional, Union

import torch
import transformers
from torch import Tensor
from transformers import set_seed

from nanogcg.utils import INIT_CHARS, find_executable_batch_size, get_nonascii_toks, mellowmax

logger = logging.getLogger("nanogcg")
if not logger.hasHandlers():
    handler = logging.StreamHandler()
    formatter = logging.Formatter(
        "%(asctime)s [%(filename)s:%(lineno)d] %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
    )
    handler.setFormatter(formatter)
    logger.addHandler(handler)
    logger.setLevel(logging.INFO)

@dataclass
class GCGConfig:
    num_steps: int = 250
    optim_str_init: Union[str, List[str]] = "x x x x x x x x x x x x x x x x x x x x"
    search_width: int = 512
    batch_size: int = None
    topk: int = 256
    n_replace: int = 1
    buffer_size: int = 0
    use_mellowmax: bool = False
    mellowmax_alpha: float = 1.0
    early_stop: bool = False
    use_prefix_cache: bool = True
    allow_non_ascii: bool = False
    filter_ids: bool = True
    add_space_before_target: bool = False
    seed: int = None
    verbosity: str = "INFO"
    success_threshold: float = 0.1  # Add success threshold for stopping criteria

@dataclass
class GCGResult:
    best_loss: float
    best_string: str
    losses: List[float]
    strings: List[str]

class AttackBuffer:
    def __init__(self, size: int):
        self.buffer = [] # elements are (loss: float, optim_ids: Tensor)
        self.size = size

    def add(self, loss: float, optim_ids: Tensor) -> None:
        if self.size == 0:
            self.buffer = [(loss, optim_ids)]
            return

        if len(self.buffer) < self.size:
            self.buffer.append((loss, optim_ids))
        else:
            self.buffer[-1] = (loss, optim_ids)

        self.buffer.sort(key=lambda x: x[0])

    def get_best_ids(self) -> Tensor:
        return self.buffer[0][1]

    def get_lowest_loss(self) -> float:
        return self.buffer[0][0]
    
    def get_highest_loss(self) -> float:
        return self.buffer[-1][0]
    
    def log_buffer(self, tokenizer):
        message = "buffer:"
        for loss, ids in self.buffer:
            optim_str = tokenizer.batch_decode(ids)[0]
            optim_str = optim_str.replace("\\", "\\\\")
            optim_str = optim_str.replace("\n", "\\n")
            message += f"\nloss: {loss}" + f" | string: {optim_str}"
        logger.info(message)

def sample_ids_from_grad(
    ids: Tensor, 
    grad: Tensor, 
    search_width: int, 
    topk: int = 256,
    n_replace: int = 1,
    not_allowed_ids: Tensor = False,
):
    """Returns `search_width` combinations of token ids based on the token gradient.

    Args:
        ids : Tensor, shape = (n_optim_ids)
            the sequence of token ids that are being optimized 
        grad : Tensor, shape = (n_optim_ids, vocab_size)
            the gradient of the GCG loss computed with respect to the one-hot token embeddings
        search_width : int
            the number of candidate sequences to return
        topk : int
            the topk to be used when sampling from the gradient
        n_replace : int
            the number of token positions to update per sequence
        not_allowed_ids : Tensor, shape = (n_ids)
            the token ids that should not be used in optimization
    
    Returns:
        sampled_ids : Tensor, shape = (search_width, n_optim_ids)
            sampled token ids
    """
    n_optim_tokens = len(ids)
    original_ids = ids.repeat(search_width, 1)

    if not_allowed_ids is not None:
        grad[:, not_allowed_ids.to(grad.device)] = float("inf")

    topk_ids = (-grad).topk(topk, dim=1).indices

    sampled_ids_pos = torch.argsort(torch.rand((search_width, n_optim_tokens), device=grad.device))[..., :n_replace]
    sampled_ids_val = torch.gather(
        topk_ids[sampled_ids_pos],
        2,
        torch.randint(0, topk, (search_width, n_replace, 1), device=grad.device)
    ).squeeze(2)

    new_ids = original_ids.scatter_(1, sampled_ids_pos, sampled_ids_val)

    return new_ids

def filter_ids(ids: Tensor, tokenizer: transformers.PreTrainedTokenizer):
    """Filters out sequeneces of token ids that change after retokenization.

    Args:
        ids : Tensor, shape = (search_width, n_optim_ids) 
            token ids 
        tokenizer : ~transformers.PreTrainedTokenizer
            the model's tokenizer
    
    Returns:
        filtered_ids : Tensor, shape = (new_search_width, n_optim_ids)
            all token ids that are the same after retokenization
    """
    ids_decoded = tokenizer.batch_decode(ids)
    filtered_ids = []

    for i in range(len(ids_decoded)):
        # Retokenize the decoded token ids
        ids_encoded = tokenizer(ids_decoded[i], return_tensors="pt", add_special_tokens=False).to(ids.device)["input_ids"][0]
        if torch.equal(ids[i], ids_encoded):
           filtered_ids.append(ids[i]) 
    
    if not filtered_ids:
        # This occurs in some cases, e.g. using the Llama-3 tokenizer with a bad initialization
        raise RuntimeError(
            "No token sequences are the same after decoding and re-encoding. "
            "Consider setting `filter_ids=False` or trying a different `optim_str_init`"
        )
    
    return torch.stack(filtered_ids)

class GCG:
    def __init__(
        self, 
        model: transformers.PreTrainedModel,
        tokenizer: transformers.PreTrainedTokenizer,
        config: GCGConfig,
    ):
        self.model = model
        self.tokenizer = tokenizer
        self.config = config

        self.embedding_layer = model.get_input_embeddings()
        self.not_allowed_ids = None if config.allow_non_ascii else get_nonascii_toks(tokenizer, device=model.device)
        self.prefix_cache = None

        self.stop_flag = False

        if model.dtype in (torch.float32, torch.float64):
            logger.warning(f"Model is in {model.dtype}. Use a lower precision data type, if possible, for much faster optimization.")

        if model.device == torch.device("cpu"):
            logger.warning("Model is on the CPU. Use a hardware accelerator for faster optimization.")

        if not tokenizer.chat_template:
            logger.warning("Tokenizer does not have a chat template. Assuming base model and setting chat template to empty.")
            tokenizer.chat_template = "{% for message in messages %}{{ message['content'] }}{% endfor %}"

        # Initialize lists to store embeddings and target IDs
        self.before_embeds_list = []
        self.after_embeds_list = []
        self.target_ids_list = []
        self.target_embeds_list = []

    def run(
        self,
        messages_list: List[Union[str, List[dict]]],
        targets_list: List[str],
    ) -> GCGResult:
        model = self.model
        tokenizer = self.tokenizer
        config = self.config

        if config.seed is not None:
            set_seed(config.seed)
            torch.use_deterministic_algorithms(True, warn_only=True)

        # Process each message-target pair
        for messages, target in zip(messages_list, targets_list):
            if isinstance(messages, str):
                messages = [{"role": "user", "content": messages}]
            else:
                messages = copy.deepcopy(messages)
        
            # Append the GCG string at the end of the prompt if location not specified
            if not any(["{optim_str}" in d["content"] for d in messages]):
                messages[-1]["content"] = messages[-1]["content"] + "{optim_str}"

            template = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True) 
            # Remove the BOS token -- this will get added when tokenizing, if necessary
            if tokenizer.bos_token and template.startswith(tokenizer.bos_token):
                template = template.replace(tokenizer.bos_token, "")
            before_str, after_str = template.split("{optim_str}")

            target = " " + target if config.add_space_before_target else target

            # Tokenize everything that doesn't get optimized
            before_ids = tokenizer([before_str], padding=False, return_tensors="pt")["input_ids"].to(model.device, torch.int64)
            after_ids = tokenizer([after_str], add_special_tokens=False, return_tensors="pt")["input_ids"].to(model.device, torch.int64)
            target_ids = tokenizer([target], add_special_tokens=False, return_tensors="pt")["input_ids"].to(model.device, torch.int64)

            # Embed everything that doesn't get optimized
            embedding_layer = self.embedding_layer
            before_embeds, after_embeds, target_embeds = [embedding_layer(ids) for ids in (before_ids, after_ids, target_ids)]

            # Store embeddings and target IDs
            self.before_embeds_list.append(before_embeds)
            self.after_embeds_list.append(after_embeds)
            self.target_ids_list.append(target_ids)
            self.target_embeds_list.append(target_embeds)

        # Now, set m_c = 1
        m_c = 1
        m = len(messages_list)

        # Initialize the attack buffer
        buffer = self.init_buffer()
        optim_ids = buffer.get_best_ids()

        losses = []
        optim_strings = []
        
        for _ in tqdm(range(config.num_steps)):
            # Compute the token gradient summed over the first m_c prompts
            optim_ids_onehot_grad = self.compute_token_gradient(optim_ids, m_c)

            with torch.no_grad():
                # Sample candidate token sequences based on the token gradient
                sampled_ids = sample_ids_from_grad(
                    optim_ids.squeeze(0),
                    optim_ids_onehot_grad.squeeze(0),
                    config.search_width,
                    config.topk,
                    config.n_replace,
                    not_allowed_ids=self.not_allowed_ids,
                )

                if config.filter_ids:
                    sampled_ids = filter_ids(sampled_ids, tokenizer)

                new_search_width = sampled_ids.shape[0]

                # For each prompt, create input_embeds
                input_embeds_list = []
                for i in range(m_c):
                    before_embeds = self.before_embeds_list[i]
                    after_embeds = self.after_embeds_list[i]
                    target_embeds = self.target_embeds_list[i]

                    if self.prefix_cache:
                        input_embeds = torch.cat([
                            self.embedding_layer(sampled_ids),
                            after_embeds.repeat(new_search_width, 1, 1),
                            target_embeds.repeat(new_search_width, 1, 1),
                        ], dim=1)
                    else:
                        input_embeds = torch.cat([
                            before_embeds.repeat(new_search_width, 1, 1),
                            self.embedding_layer(sampled_ids),
                            after_embeds.repeat(new_search_width, 1, 1),
                            target_embeds.repeat(new_search_width, 1, 1),
                        ], dim=1)
                    input_embeds_list.append(input_embeds)

                # Compute loss over all candidate sequences, summed over prompts
                loss = self.compute_candidates_loss(input_embeds_list, self.target_ids_list[:m_c])

                current_loss = loss.min().item()
                optim_ids = sampled_ids[loss.argmin()].unsqueeze(0)

                # Update the buffer based on the loss
                losses.append(current_loss)
                if buffer.size == 0 or current_loss < buffer.get_highest_loss():
                    buffer.add(current_loss, optim_ids)

            optim_ids = buffer.get_best_ids()
            optim_str = tokenizer.batch_decode(optim_ids)[0]
            optim_strings.append(optim_str)

            buffer.log_buffer(tokenizer)                

            # Check success condition and increment m_c
            if self.success_condition(optim_ids, m_c) and m_c < m:
                m_c += 1
                logger.info(f"Incremented m_c to {m_c}")
                # else:
                #     logger.info("All prompts succeeded. Stopping optimization.")
                #     break

        min_loss_index = losses.index(min(losses)) 

        result = GCGResult(
            best_loss=losses[min_loss_index],
            best_string=optim_strings[min_loss_index],
            losses=losses,
            strings=optim_strings,
        )

        return result

    def compute_token_gradient(
        self,
        optim_ids: Tensor,
        m_c: int,
    ) -> Tensor:
        model = self.model
        embedding_layer = self.embedding_layer

        # Create the one-hot encoding matrix of our optimized token ids
        optim_ids_onehot = torch.nn.functional.one_hot(optim_ids, num_classes=embedding_layer.num_embeddings)
        optim_ids_onehot = optim_ids_onehot.to(model.device, model.dtype)
        optim_ids_onehot.requires_grad_()

        optim_embeds = optim_ids_onehot @ embedding_layer.weight  # (1, num_optim_tokens, embed_dim)

        # Initialize gradient to zero
        optim_ids_onehot_grad = torch.zeros_like(optim_ids_onehot)

        # For each prompt up to m_c, compute the loss and gradient
        for idx in range(m_c):
            before_embeds = self.before_embeds_list[idx]
            after_embeds = self.after_embeds_list[idx]
            target_ids = self.target_ids_list[idx]
            target_embeds = self.target_embeds_list[idx]

            if self.prefix_cache:
                input_embeds = torch.cat([optim_embeds, after_embeds, target_embeds], dim=1)
                output = model(inputs_embeds=input_embeds, past_key_values=self.prefix_cache)
            else:
                input_embeds = torch.cat([before_embeds, optim_embeds, after_embeds, target_embeds], dim=1)
                output = model(inputs_embeds=input_embeds)

            logits = output.logits

            # Shift logits so token n-1 predicts token n
            shift = input_embeds.shape[1] - target_ids.shape[1]
            shift_logits = logits[..., shift-1:-1, :].contiguous()  # (1, num_target_ids, vocab_size)
            shift_labels = target_ids

            if self.config.use_mellowmax:
                label_logits = torch.gather(shift_logits, -1, shift_labels.unsqueeze(-1)).squeeze(-1)
                loss = mellowmax(-label_logits, alpha=self.config.mellowmax_alpha, dim=-1)
            else:
                loss = torch.nn.functional.cross_entropy(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))

            # Compute gradient with retain_graph=True
            grad = torch.autograd.grad(outputs=[loss], inputs=[optim_ids_onehot], retain_graph=True)[0]

            optim_ids_onehot_grad += grad

            del output
            gc.collect()
            torch.cuda.empty_cache()

        return optim_ids_onehot_grad


    def compute_candidates_loss(
        self,
        input_embeds_list: List[Tensor],
        target_ids_list: List[Tensor],
    ) -> Tensor:
        all_loss = None
        for input_embeds, target_ids in zip(input_embeds_list, target_ids_list):
            # Compute loss for this prompt
            loss = find_executable_batch_size(self._compute_loss_for_candidates)(
                input_embeds, target_ids
            )
            if all_loss is None:
                all_loss = loss
            else:
                all_loss += loss

        return all_loss

    def _compute_loss_for_candidates(
        self,
        batch_size: int,
        input_embeds: Tensor,
        target_ids: Tensor,
    ) -> Tensor:
        all_loss = []
        prefix_cache_batch = []

        for i in range(0, input_embeds.shape[0], batch_size):
            with torch.no_grad():
                input_embeds_batch = input_embeds[i:i+batch_size]
                current_batch_size = input_embeds_batch.shape[0]

                if self.prefix_cache:
                    if not prefix_cache_batch or current_batch_size != batch_size:
                        prefix_cache_batch = [[x.expand(current_batch_size, -1, -1, -1) for x in self.prefix_cache[i]] for i in range(len(self.prefix_cache))]

                    outputs = self.model(inputs_embeds=input_embeds_batch, past_key_values=prefix_cache_batch)
                else:
                    outputs = self.model(inputs_embeds=input_embeds_batch)

                logits = outputs.logits

                tmp = input_embeds_batch.shape[1] - target_ids.shape[1]
                shift_logits = logits[..., tmp-1:-1, :].contiguous()
                shift_labels = target_ids.repeat(current_batch_size, 1)
                
                if self.config.use_mellowmax:
                    label_logits = torch.gather(shift_logits, -1, shift_labels.unsqueeze(-1)).squeeze(-1)
                    loss = mellowmax(-label_logits, alpha=self.config.mellowmax_alpha, dim=-1)
                else:
                    loss = torch.nn.functional.cross_entropy(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1), reduction="none")

                loss = loss.view(current_batch_size, -1).mean(dim=-1)
                all_loss.append(loss)

                del outputs
                gc.collect()
                torch.cuda.empty_cache()

        return torch.cat(all_loss, dim=0)

    def success_condition(
        self,
        optim_ids: Tensor,
        m_c: int,
    ) -> bool:
        model = self.model
        embedding_layer = self.embedding_layer
        threshold = self.config.success_threshold

        optim_embeds = embedding_layer(optim_ids)

        for idx in range(m_c):
            before_embeds = self.before_embeds_list[idx]
            after_embeds = self.after_embeds_list[idx]
            target_ids = self.target_ids_list[idx]
            target_embeds = self.target_embeds_list[idx]

            if self.prefix_cache:
                input_embeds = torch.cat([optim_embeds, after_embeds, target_embeds], dim=1)
                output = model(inputs_embeds=input_embeds, past_key_values=self.prefix_cache)
            else:
                input_embeds = torch.cat([before_embeds, optim_embeds, after_embeds, target_embeds], dim=1)
                output = model(inputs_embeds=input_embeds)

            logits = output.logits

            # Shift logits so token n-1 predicts token n
            shift = input_embeds.shape[1] - target_ids.shape[1]
            shift_logits = logits[..., shift-1:-1, :].contiguous()
            shift_labels = target_ids

            if self.config.use_mellowmax:
                label_logits = torch.gather(shift_logits, -1, shift_labels.unsqueeze(-1)).squeeze(-1)
                loss = mellowmax(-label_logits, alpha=self.config.mellowmax_alpha, dim=-1)
            else:
                loss = torch.nn.functional.cross_entropy(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))

            if loss.item() > threshold:
                return False

            del output
            gc.collect()
            torch.cuda.empty_cache()

        return True

    def init_buffer(self) -> AttackBuffer:
        # ... [Modify this method to align with batch_size handling]

        model = self.model
        tokenizer = self.tokenizer
        config = self.config

        logger.info(f"Initializing attack buffer of size {config.buffer_size}...")

        # Create the attack buffer and initialize the buffer ids
        buffer = AttackBuffer(config.buffer_size)

        if isinstance(config.optim_str_init, str):
            init_optim_ids = tokenizer(config.optim_str_init, add_special_tokens=False, return_tensors="pt")["input_ids"].to(model.device)
            if config.buffer_size > 1:
                init_buffer_ids = tokenizer(INIT_CHARS, add_special_tokens=False, return_tensors="pt")["input_ids"].squeeze().to(model.device)
                init_indices = torch.randint(0, init_buffer_ids.shape[0], (config.buffer_size - 1, init_optim_ids.shape[1]))
                init_buffer_ids = torch.cat([init_optim_ids, init_buffer_ids[init_indices]], dim=0)
            else:
                init_buffer_ids = init_optim_ids
                
        else:  # Assume list
            if (len(config.optim_str_init) != config.buffer_size):
                logger.warning(f"Using {len(config.optim_str_init)} initializations but buffer size is set to {config.buffer_size}")
            try:
                init_buffer_ids = tokenizer(config.optim_str_init, add_special_tokens=False, return_tensors="pt")["input_ids"].to(model.device)
            except ValueError:
                logger.error("Unable to create buffer. Ensure that all initializations tokenize to the same length.")

        true_buffer_size = max(1, config.buffer_size) 

        # Compute the loss on the initial buffer entries over the first m_c message-target pair
        m_c = 1  # Start with first message-target pair
        all_losses = []

        for idx in range(m_c):
            before_embeds = self.before_embeds_list[idx]
            after_embeds = self.after_embeds_list[idx]
            target_ids = self.target_ids_list[idx]
            target_embeds = self.target_embeds_list[idx]

            if self.prefix_cache:
                init_buffer_embeds = torch.cat([
                    self.embedding_layer(init_buffer_ids),
                    after_embeds.repeat(true_buffer_size, 1, 1),
                    target_embeds.repeat(true_buffer_size, 1, 1),
                ], dim=1)
            else:
                init_buffer_embeds = torch.cat([
                    before_embeds.repeat(true_buffer_size, 1, 1),
                    self.embedding_layer(init_buffer_ids),
                    after_embeds.repeat(true_buffer_size, 1, 1),
                    target_embeds.repeat(true_buffer_size, 1, 1),
                ], dim=1)

            init_buffer_losses = find_executable_batch_size(self._compute_loss_for_candidates)(
                init_buffer_embeds, target_ids
            )

            all_losses.append(init_buffer_losses)

        # Sum the losses over the first m_c prompts
        total_loss = sum(all_losses)

        # Populate the buffer
        for i in range(true_buffer_size):
            buffer.add(total_loss[i], init_buffer_ids[[i]])
        
        buffer.log_buffer(tokenizer)

        logger.info("Initialized attack buffer.")
        
        return buffer

# A wrapper around the GCG run method that provides a simple API
def run(
    model: transformers.PreTrainedModel,
    tokenizer: transformers.PreTrainedTokenizer,
    messages_list: List[Union[str, List[dict]]],
    targets_list: List[str],
    config: Optional[GCGConfig] = None, 
) -> GCGResult:
    """Generates a single optimized string using GCG. 

    Args:
        model: The model to use for optimization.
        tokenizer: The model's tokenizer.
        messages_list: A list of conversations to use for optimization.
        targets_list: A list of target generations corresponding to each message.
        config: The GCG configuration to use.

    Returns:
        A GCGResult object that contains losses and the optimized strings.
    """
    if config is None:
        config = GCGConfig()
    
    logger.setLevel(getattr(logging, config.verbosity))
    
    gcg = GCG(model, tokenizer, config)
    result = gcg.run(messages_list, targets_list) 
    return result
